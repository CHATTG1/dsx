{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telco Churn (Livy) - Remote HDP\n",
    "\n",
    "IBM DSX Local provides the interface for Python notebooks to work with an existing remote Spark through HTTP connection and user-friendly sparkmagic commands. This sample notebook shows how to work with remote Spark using the Livy Spark kernel.\n",
    "\n",
    "The installation of the remote Spark in this sample is using Horton Data Platform (HDP), which utilizes Livy HTTP REST API. Livy is an open source REST interface for interacting with [Apache Spark](http://spark.apache.org) from anywhere. It supports executing snippets of code or programs in a Spark context that runs locally or in [Apache Hadoop YARN](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html).\n",
    "\n",
    "This notebook runs on Python2 with Livy Spark.\n",
    "\n",
    "![CRISP-DM](https://raw.githubusercontent.com/dhananjaymehta/IoTtrucking/master/HDP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load sparkmagic\n",
    "\n",
    "Sparkmagic is a set of tools for interactively working with remote Spark clusters through Livy, a Spark REST server, in Jupyter notebooks. The Sparkmagic project includes a set of magics for interactively running Spark code in multiple languages, as well as some kernels that you can use to turn Jupyter into an integrated Spark environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success configuring sparkmagic livy.\n"
     ]
    }
   ],
   "source": [
    "%load_ext sparkmagic.magics\n",
    "import dsx_core_utils\n",
    "dsx_core_utils.setup_livy_sparkmagic()\n",
    "#%reload_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Create a connection to remote Spark\n",
    "Creating a spark livy connection to remote Hadoop Cluster\n",
    "![CRISP-DM](https://raw.githubusercontent.com/dhananjaymehta/IoTtrucking/master/ambari.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a livy session (this might not be needed after v1.2)\n",
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the path for server and demo\n",
    "demo_path = '/tmp/'\n",
    "hdfs_server = 'http://HDP_Cluster:50070/webhdfs/v1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. View the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Chustomer data\n",
    "#!curl -i -L \"http://HDP_Cluster:50070/webhdfs/v1/user/customer.csv?op=OPEN\" | tail -n 5\n",
    "!wget -i -L \"https://raw.githubusercontent.com/hortonworks-gallery/dsx/master/Telco-Churn-Prediction/dataset/customer.csv\" | tail -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 20079  100 20079    0     0   286k      0 --:--:-- --:--:-- --:--:--  286k\n",
      "3821,\"T\"\n",
      "3822,\"T\"\n",
      "3823,\"T\"\n",
      "3824,\"T\"\n",
      "3825,\"T\"\n"
     ]
    }
   ],
   "source": [
    "# Churn data\n",
    "#!curl -i -L \"http://HDP_Cluster:50070/webhdfs/v1/user/dsx_datasets/churn.csv?op=OPEN\" | tail -n 5\n",
    "!wget -i -L \"https://raw.githubusercontent.com/hortonworks-gallery/dsx/master/Telco-Churn-Prediction/dataset/churn.csv\" | tail -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting DSX Configuration File.\n",
    "Setting DSX configuration file that will be used in HDP cluster for for connecting to DSX Cluster. This file contains following details -\n",
    "- DSX_PROJECT_NAME\n",
    "- DSX_PROJECT_ID\n",
    "- DSX_TOKEN\n",
    "- DSX_USER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dsx_core_utils import hdfs_util\n",
    "\n",
    "# uploading data file to be used to create model - > this has been taken care of \n",
    "#hdfs_util.upload_file(hdfs_server,\"./customer.csv\" ,\"/tmp/customer.csv\") \n",
    "\n",
    "# uploading config file for dsx credentials and data -> This has to be done\n",
    "hdfs_util.upload_dsx_config_file(hdfs_server, demo_path + \"dsx_config_file.txt\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# now that we are starting to run in remote spark. We must have a reference to the same demo path in order to know where to get the files back from. \n",
    "demo_path = '/tmp/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+--------+----------+---------+---------+------------+-------------+------+-------+---------+-------------+--------------------+------+--------+\n",
      "| ID|Gender|Status|Children|Est Income|Car Owner|      Age|LongDistance|International| Local|Dropped|Paymethod|LocalBilltype|LongDistanceBilltype| Usage|RatePlan|\n",
      "+---+------+------+--------+----------+---------+---------+------------+-------------+------+-------+---------+-------------+--------------------+------+--------+\n",
      "|  1|     F|     S|     1.0|   38000.0|        N|24.393333|       23.56|          0.0|206.08|    0.0|       CC|       Budget|      Intnl_discount|229.64|     3.0|\n",
      "|  6|     M|     M|     2.0|   29616.0|        N|49.426667|       29.78|          0.0|  45.5|    0.0|       CH|    FreeLocal|            Standard| 75.29|     2.0|\n",
      "|  8|     M|     M|     0.0|   19732.8|        N|50.673333|       24.81|          0.0| 22.44|    0.0|       CC|    FreeLocal|            Standard| 47.25|     3.0|\n",
      "| 11|     M|     S|     2.0|     96.33|        N|56.473333|       26.13|          0.0| 32.88|    1.0|       CC|       Budget|            Standard| 59.01|     1.0|\n",
      "| 14|     F|     M|     2.0|   52004.8|        N|    25.14|        5.03|          0.0| 23.11|    0.0|       CH|       Budget|      Intnl_discount| 28.14|     1.0|\n",
      "+---+------+------+--------+----------+---------+---------+------------+-------------+------+-------+---------+-------------+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-----+\n",
      "| ID|CHURN|\n",
      "+---+-----+\n",
      "|  1|    T|\n",
      "|  6|    F|\n",
      "|  8|    F|\n",
      "| 11|    F|\n",
      "| 14|    F|\n",
      "+---+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "#sc = SparkContext()\n",
    "# Customer Information\n",
    "customer = SQLContext(sc).read.csv('./customer.csv', header='true', inferSchema = 'true')\n",
    "#customer = SQLContext(sc).read.csv('hdfs://HDP_Cluster:8020/user/customer.csv', header=True, inferSchema=True)\n",
    "customer.show(5)\n",
    "\n",
    "#Churn information    \n",
    "customer = SQLContext(sc).read.csv('./churn.csv', header='true', inferSchema = 'true')\n",
    "#customer_churn = SQLContext(sc).read.csv('hdfs://HDP_Cluster:8020/user/churn.csv', header=True, inferSchema=True)\n",
    "customer_churn.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performing Data Wrangling \n",
    "In this section we - \n",
    "- Join Customer and Churn datasets\n",
    "- modify names of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "data =customer.join(customer_churn,customer['ID']==customer_churn['ID']).select(customer['*'],customer_churn['CHURN'])\n",
    "data = data.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build a Classification Model\n",
    "This model is built on HDP, any calculations and computations performed by Spark is utilizing the resources of HDP cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "import shutil\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model\n",
    "\n",
    "\n",
    "#df_data = spark.read.format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat').option('header', 'true').option('inferSchema', 'true').load(data_file_path)\n",
    "\n",
    "# Importing data to dataframe\n",
    "\n",
    "splitted_data = data.randomSplit([0.8, 0.18, 0.02], 24)\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "predict_data = splitted_data[2]\n",
    "\n",
    "print \"Number of training records: \" + str(train_data.count())\n",
    "print \"Number of testing records : \" + str(test_data.count())\n",
    "print \"Number of prediction records : \" + str(predict_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Prepare string variables so that they can be used by the decision tree algorithm\n",
    "# StringIndexer encodes a string column of labels to a column of label indices\n",
    "SI1 = StringIndexer(inputCol='Gender', outputCol='GenderEncoded')\n",
    "SI2 = StringIndexer(inputCol='Status',outputCol='StatusEncoded')\n",
    "SI3 = StringIndexer(inputCol='CarOwner',outputCol='CarOwnerEncoded')\n",
    "SI4 = StringIndexer(inputCol='Paymethod',outputCol='PaymethodEncoded')\n",
    "SI5 = StringIndexer(inputCol='LocalBilltype',outputCol='LocalBilltypeEncoded')\n",
    "SI6 = StringIndexer(inputCol='LongDistanceBilltype',outputCol='LongDistanceBilltypeEncoded')\n",
    "\n",
    "#encode the Label column\n",
    "labelIndexer = StringIndexer(inputCol='CHURN', outputCol='label').fit(train_data)\n",
    "\n",
    "\n",
    "# Pipelines API requires that input variables are passed in  a vector\n",
    "assembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \"LocalBilltypeEncoded\", \\\n",
    "                                       \"LongDistanceBilltypeEncoded\", \"Children\", \"EstIncome\", \"Age\", \"LongDistance\", \"International\", \"Local\",\\\n",
    "                                      \"Dropped\",\"Usage\",\"RatePlan\"], outputCol=\"features\")\n",
    "\n",
    "# instantiate the algorithm, take the default settings\n",
    "rf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n",
    "\n",
    "# Create a pipeline \n",
    "pipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6,labelIndexer, assembler, rf, labelConverter])\n",
    "\n",
    "# Build model. The fitted model from a Pipeline is a PipelineModel, \n",
    "# which consists of fitted models and transformers, corresponding to the pipeline stages.\n",
    "model_rf = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing & Validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# testing the model accuracy\n",
    "predictions = model_rf.transform(test_data)\n",
    "evaluatorRF = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluatorRF.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "results = model_rf.transform(test_data)\n",
    "results=results.select(results[\"ID\"],results[\"CHURN\"],results[\"label\"],results[\"predictedLabel\"],results[\"prediction\"],results[\"probability\"])\n",
    "results.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving model Pipeline\n",
    "The model that was developed on hadoop cluster is now saved Local file system of the Spark Node. This File is than moved from local filesystem to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "import os\n",
    "os.chdir(\"/tmp\")\n",
    "\n",
    "# Save model\n",
    "pipeline_model_name = \"PipelineModel_TelcoChurn_Spark_v1.0\"\n",
    "pipeline_model_path = \"/tmp/\"+pipeline_model_name\n",
    "\n",
    "model_rf.save(pipeline_model_path)\n",
    "\n",
    "from subprocess import Popen, PIPE, STDOUT \n",
    "\n",
    "cmd = \"hdfs dfs -copyToLocal \"+pipeline_model_path +\" \" +  pipeline_model_path\n",
    "p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)\n",
    "output = p.stdout.read()\n",
    "print (output)\n",
    "\n",
    "# compress into tar.gz files, \n",
    "# IMPORTANT: please compress exactly in the structure here. The archived model must not have ANY prefix directories in the archive. \n",
    "shutil.make_archive( base_name =pipeline_model_name, format= 'gztar', root_dir = pipeline_model_name +'/',  base_dir  =  './' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "dsx_data_path = demo_path+'dsx_config_file.txt'\n",
    "\n",
    "cmd = \"hdfs dfs -copyToLocal \"+dsx_data_path +\" \" +  dsx_data_path\n",
    "p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)\n",
    "output = p.stdout.read()\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "#pipeline_file_path = '/tmp/' + pipeline_name +'.tar.gz'\n",
    "pipeline_model_file_path ='/tmp/'+pipeline_model_name +'.tar.gz'\n",
    "\n",
    "dsx_data = [line.rstrip('\\n') for line in open(dsx_data_path)]\n",
    "projectGuid = dsx_data[0]\n",
    "projectId = dsx_data[1]\n",
    "auth_header = dsx_data[2] \n",
    "user_id = dsx_data[3]\n",
    "\n",
    "print dsx_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Deploying model on DSX\n",
    "Once the model has been created it is sent over HTTP to DSX Cluster where this model is hosted against a REST endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "import json\n",
    "import requests \n",
    "#NOTE: requires request library dependency, or any other way of submitting a REST call\n",
    "\n",
    "headers = {\n",
    "    'Authorization': auth_header\n",
    "}\n",
    "author = {\n",
    "    \"name\": \"Project Owner\",\n",
    "    \"projectName\" : projectGuid,\n",
    "    \"uid\" : user_id\n",
    "}\n",
    "\n",
    "r = requests.post('https://load_balancer_ip/v3/project/' + author['projectName'] + '/models/saveSparkModel',\n",
    "                  files={\n",
    "                      'pipelineModelFile': open(pipeline_model_file_path,'rb'),\n",
    "                      \"trainingDataSchema\": train_data.schema.json(),\n",
    "                      \"pipelineModelName\": \"Name_the_model\",\n",
    "                      \"modelType\" : \"spark-2.1\",\n",
    "                      \"labelColumnName\" : 'CHURN',\n",
    "                      \"runtime\": 'Python35',\n",
    "                      \"algorithmType\": 'Classification',\n",
    "                      \"algorithm\": 'PipelineModel'\n",
    "                  },\n",
    "                  headers=headers,\n",
    "                  verify=False)\n",
    "\n",
    "print(r.status_code)\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Testing the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "!curl -k -X GET https://Load_balancer_ip/v2/identity/token -H \"username: user\" -H \"password: password\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing over UI\n",
    "1. Save the notebook and switch to the **Models** tab of the project or go under **Model Management** \n",
    "2. Under **Models**, find and click your model. Now, click on publish. \n",
    "3. Now use the **Test API** option to test the model on UI.\n",
    "\n",
    "You can use the following data for testing: ID=99, Gender=M, Status=S, Children=0, Est Income=60000, Car Owner=Y, Age=34, LongDistance=68, International=50, Local=100, Dropped=0, Paymethod=CC, LocalBilltype=Budget, LongDistanceBilltype=Intnl_discount, Usage=334, RatePlan=3\n",
    "![CRISP-DM](https://raw.githubusercontent.com/dhananjaymehta/IoTtrucking/master/Test_GUI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing over CURL\n",
    "This step is to demonstrate that you can make an external REST API call to test the model. Create and execute this command to invoke the model remotely from terminal or your program: \n",
    "!curl -i -k -X POST <Scoring Endpoint> -d '{fields}' \"content-type: application/json\" -H \"authorization: Bearer <generate bearer token>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model\n",
    "!curl -i -k -X POST scoring_endpoint -d '{\"ID\":99,\"Gender\":\"M\",\"Status\":\"S\",\"Children\":0,\"EstIncome\":60000,\"CarOwner\":\"Y\",\"Age\":34,\"LongDistance\":68,\"International\":50,\"Local\":100,\"Dropped\":0,\"Paymethod\":\"CC\",\"LocalBilltype\":\"Budget\",\"LongDistanceBilltype\":\"Intnl_discount\",\"Usage\":334,\"RatePlan\":3}' -H \"content-type: application/json\" -H \"authorization: Bearer Bearer_Token\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
